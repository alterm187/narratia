{
  "slug": "openai-safety-1-wrzesnia",
  "title": {
    "pl": "OpenAI Safety Team - Small Group, Big Decisions",
    "en": "OpenAI Safety Team - Small Group, Big Decisions"
  },
  "excerpt": {
    "pl": "It was not the capabilities of GPT5 that drew my attention while watching the OpenAI livestream. Listening to Saachi Jain, who leads the Safety Training team at OpenAI, I realized that...",
    "en": "It was not the capabilities of GPT5 that drew my attention while watching the OpenAI livestream. Listening to Saachi Jain, who leads the Safety Training team at OpenAI, I realized that..."
  },
  "content": {
    "pl": "It was not the capabilities of GPT5 that drew my attention while watching the OpenAI livestream. Listening to Saachi Jain, who leads the Safety Training team at OpenAI, I realized that...\n\n... the definitions of even the very expected behaviors of most powerful AI systems are in hands of just a small group of people, not mentioning the actually achieved behaviors.\n\nSo will the model provide information necessary for creating biological weapon or not, it depends on how this team formulated their goals and priorities. One team! Size unknown. It might be a team of very smart people but...\n\nThe ideas of superalignment that Sutskever and Amodei had were at least more ambitious than just practical UX decisions on model behavior.\n\nWhat's worse, it seems that recently published America's AI strategy kind of suggests that maybe this is not so important as many other models from different countries are not following any alignment rules anyway.",
    "en": "It was not the capabilities of GPT5 that drew my attention while watching the OpenAI livestream. Listening to Saachi Jain, who leads the Safety Training team at OpenAI, I realized that...\n\n... the definitions of even the very expected behaviors of most powerful AI systems are in hands of just a small group of people, not mentioning the actually achieved behaviors.\n\nSo will the model provide information necessary for creating biological weapon or not, it depends on how this team formulated their goals and priorities. One team! Size unknown. It might be a team of very smart people but...\n\nThe ideas of superalignment that Sutskever and Amodei had were at least more ambitious than just practical UX decisions on model behavior.\n\nWhat's worse, it seems that recently published America's AI strategy kind of suggests that maybe this is not so important as many other models from different countries are not following any alignment rules anyway."
  },
  "date": "2025-09-01",
  "category": "thoughts",
  "tags": ["AI", "safety", "alignment"],
  "author": "Sebastian Proba",
  "featured": false
}
